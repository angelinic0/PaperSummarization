
Sequential learning, also referred to as continual, incremental, or lifelong learning , studies the
problem of learning a sequence of tasks, one at a time, without access to the training data of previous
or future tasks. When learning a new task, a key challenge in this context is how to avoid catastrophic
interference with the tasks learned previously . Some methods
exploit an additional episodic memory to store a small amount of previous tasks data to regularize
future task learning . Others store previous tasks models and at test time,
select one model or merge the models . In
contrast, in this work we are interested in the challenging situation of learning a sequence of tasks
without access to any previous or future task data and restricted to a fixed model capacity, as also
studied in|Kirkpatrick et al.|;|Mallya & Lazebnik
; . This scenario not only has many practical benefits, including privacy an
scalability, but also resembles more closely how the mammalian brain learns tasks over time.
The mammalian brain is composed of billions of neurons. Yet at any given time, information is
represented by only a few active neurons resulting in a sparsity of 90-95% . In
neural biology, lateral inhibition describes the process where an activated neuron reduces the activity
of its weaker neighbors. This creates a powerful decorrelated and com epresentation with
minimum interference between different input patterns in the brain . This is in stark
'The code is available at https: //github.com/rahafaljundi/Selfles
Sequential-Learning
Published as a conference paper at ICLR 2019
11
Dec 2POOOO : OC
 Represantation sparsity
Figure 1: The difference between parameter sparsity  in a simple two tasks
case. First layer indicates input patterns. Learning the first task utilizes parts indicated in red. Task 2 has
different input patterns and uses parts shown in green. Orange indicates changed neurons activations as a result
of the second task. In , when an example from the first task is encountered again, the activations of the first
layer will not be affected by the changes, however, the second and later layer activations are changed. Such
interference is largely reduced when imposing sparsity on the representation .
contrast with artificial neural networks, which typically learn dense representations that are highly
entangled Such an entangled representation is quite sensitive to changes in
the input patterns, in that it responds differently to input patterns with only small variations nc
suggests that an overlapped representation plays a crucial role in Pata forgetting and
reducing this overlap would result in a reduced interference. [Cogswell et al.] 1.|  show that when
the amount of overfitting in a neural network is reduced, the - Rosswet ata corre cite is also reduced.
As such, learning a disentangled representation is more powerful and less vulnerable to catastrophic
interference. However, if the learned disentangled representation at a given task is not sparse, only
little capacity is left for learning new tasks. This would in turn result in either an underfitting to the
new tasks or again a forgetting of previous tasks. In contrast, a sparse and decorrelated representation
would lead to a powerful representation and at the same time enough free neurons that can be changed
without interference with the neural activations learned for the previous tasks.
In general, sparsity in neural networks can be thought of either in terms of the network parameters
or in terms of the representation . In this paper we postulate, and confirm
experimentally, that a sparse and decorrelated representation is preferable over parameter sparsity
in a sequential learning scenario. There are two arguments for this: first, a sparse representation is
less sensitive to new and different patterns  and second, the training
procedure of the new tasks can use the free neurons leading to less interference with the previous
tasks, hence reducing forgetting. In contrast, when the effective parameters are spread among
different neurons, changing the ineffective ones would change the function of their corresponding
neurons and hence interfere with previous tasks . Based on these observations,
we propose a new regularizer that exhibits a behavior similar to the lateral inhibition in biological
neurons. The main idea of our regularizer is to penalize neurons that are active at the same time.
This leads to more sparsity and a decorrelated representation. However, complex tasks may actually
require multiple active neurons in a layer at the same time to learn a strong representation. Therefore,
our regularizer, Sparse coding through Local Neural Inhibition and Discounting , only
penalizes neurons locally. Furthermore, we dont want inhibition to affect previously learned tasks,
even if later tasks use neurons from earlier tasks. An important component of SLNID is thus
to discount inhibition from/to neurons which have high neuron importance
a new concept that
we introduce a analogy to parameter importance (Kirkpatrick et al. 2016} |Zenke et al.|
Aljundi et al.}/2017). When combined with a state-of-the-art important parameters preservation
method , our proposed regularizer leads to sparse and
decorrelated representations which improves the lifelong learning performance.
Our contribution is threefold. First, we direct attention to Selfless Sequential Learning and study
a diverse set of representation based regularizers, parameter based regularizers, as well as sparsity
inducing activation functions to this end. These have not been studied extensively in the lifelong
learning literature before. Second, we propose a novel regularizer, SLNID, which is inspired by
lateral inhibition in the brain. Third, we show that our proposed regularizer consistently outperforms
alternatives on three diverse datasets  and we compare to
and outperform state-of-the-art LLL approaches on an 8-task object classification challenge. SLNID
can be applied to different regularization based LLL approaches, and we show experiments with
MAS .
In the following, we first discuss related LLL approaches and different regularization criteria from a
LLL perspective . We proceed by introducing Selfless Sequential Learning and detailing
our novel regularizer (Section p. Section[4|describes our experimental evaluation, while Section5]
concludes the paper.
Published as a conference paper at ICLR 2019
2 RELATED WORK
The goal in lifelong learning is to learn a sequence of tasks without catastrophic forgetting of previ-
ously learned ones . One can identify different approaches to introducing
lifelong learning in neural networks. Here, we focus on learning a sequence of tasks using a fixed
model capacity, i.e. with a fixed architecture and fixed number of parameters. Under this setting,
methods either follow a pseudo rehearsal approach, i.e. using the new task data to approximate the
performance of the previous task , or aim at identifying the
important parameters used by the current set of t: ind penalizing c 's to those parameters by
new tasks (Kirkpatrick etal 2
2018). To identify the important parameters for a given task, Elastic Weight Consolida-
tion  uses an approximation of the Fisher information matrix computed after
training a given task.  suggest a network reparameterization to obtain a better diagonal
approximation of the Fisher Information matrix of the network parameters. Path Integral
7) estimates the importance of the network parameters while learning a given task by
accumulating the contribution of each parameter to the change in the loss.
suggest a KL-divergence based generalization of Elastic Weight Consolidation and Path Integral.
Memory Aware Synapses  estimates the importance of the parameters in an
online manner without supervision by measuring the sensitivity of the learned function to small
perturbations on the parameters. This method is less sensitive to the data distribution shift, and a
local version proposed by the authors resembles applying Hebb rule  to consolidate the
important parameters, making it more biologically plausible.
A common drawback of all the above methods is that learning a task could utilize a good portion of
the network capacity, leaving few "free" neurons to be adapted by the new task. This in turn leads to
inferior performance on the newly learned tasks or forgetting the previously learned ones, as we will
show in the experiments. Hence, we study the role of sparsity and representation decorrelation in
sequential learning. This aspect has not received much attention in the literature yet. Very recently,
[2018) proposed to overcome catastrophic forgetting through learned hard attention
masks for each task with L1 regularization imposed on the accumulated hard attention masks. This
comes closer to our approach although we study and propose a regularization scheme on the learned
representation.
The concept of reducing the representation overlap has been suggested before in early attempts
towards overcoming catastrophic forgetting in neural networks . This has led to several
methods with the goal of orthogonalizing the activations (Frenchj [1992}|1994} [Kruschke| [T992}
1993} |Sloman & Rumelhart . However, these approaches are mainly designed for specific
architectures and activation functions, which makes it hard to integrate them in recent neural network
structures.
The sparsification of neural networks has mostly been studied for compression. SVD decomposition
can be applied to reduce the number of effective parameters (Xue et al.| . However, there is
no guarantee that the training procedure converges to a low rank weight matrix. Other works iterate
between pruning and retraining of a neural network as a post processing step 01
el 6 {2017} 2017). While compressing a neural network by
removing parameters leads to a sparser neural network, this does not necessarily lead to a sparser
representation. Indeed, a weight vector can be highly sparse but spread among the different neurons.
This reduces the effective size of a neural network, from a compression point of view, but it would
not be beneficial for later tasks as most of the neurons are already occupied by the current set of tasks.
In our experiments, we show the difference between using a sparse penalty on the representation
versus applying it to the weights.
3 SELFLESS SEQUENTIAL LEARNING
One of the main challenges in single model sequential learning is to have capacity to learn new
tasks and at the same time avoid catastrophic forgetting of previous tasks as a result of learning new
tasks. In order to prevent catastrophic forgetting, importance weight based methods such as EWC
 or MAS introduce an importance weight ; for each
 P ght
parameter 6), in the network. While these methods differ in how to estimate the important parameters,
Published as a conference paper at ICLR 2019
all of them penalize changes to pent parameters when learning a new task T), using Lz penalty:
Ty: min >> Ls Lymn; f

k
m=1
where 9~1 = {6?~'} are the optimal parameters learned so far, i.e. before the current task. {7m}
is the set of M training inputs, with { f} and {ym } the corresponding predicted and desired
outputs, respectively. Aq is a trade-off parameter between the new task objective
and the changes
on the important parameters, i.e. the amount of forgetting.
In this work we introduce an additional regularizer Rss, which encourages sparsity in the activations
A, = {hi} for each hi L
1
Ti: min >> e Lym; f  + ro DM (67
02-1)
+ Asst >> Resi 
m=1 7
Assi and Ae are trade-off parameters that control ig contribution of each term. When training the
first task , Q;, = 0.
3.1 SPARSE CODING THROUGH NEURAL INHIBITION 
Now we describe how we obtain a sparse and decorrelated representation. In the literature sparsity has
been proposed by |Glorot et al] ) to be combined with the rectifier activation function  to
control unbounded activations and to increase sparsity. They minimize the L; norm of the activations
. However, L; norm imposes an equal penalty
on all the active neurons leading to small activation magnitude across the network.
Learning a decorrelated representation has been explored before with the goal of reducing overfitting.
This is usually done by minimizing the Frobenius norm of the covariance matrix corrected by the
diagonal, as in|Cogswell et al.|. Such a penalty results in a decorrelated
representation but with activations that are mostly close to a non zero mean value. We merge the two
objectives of sparse and decorrelated representation resulting in the following objective:
Rsy1
ij m
where we consider a hidden layer / with activations H; = {h?"} for a set of inputs X = {x,,,} and
i,j 1,..,.N running over all N neurons in the hidden layer. This formula differs from minimizing
the Frobenius norm of the covariance matrix in two simple yet important aspects:
 In the case of a ReLU activation function, used in most modern architectures, a neuron is active if
its output is larger than zero, and zero otherwise. By assuming a close to zero mean of the activations,
bi, ~ OVi
1,..,.N, we minimize the correlation between any two active neurons.
 By evaluating the derivative of the presented segulanies w.r.t. the activation, we get:
ORsxz
_ nn A
ann 7M )
i.e., each active neuron receives a penalty from every other active neuron that corresponds to that
other neurons activation magnitude. In other words, if a neuron fires, with a high activation value,
for a given example, it will suppress firing of other neurons for that same example. Hence, this results
in a decorrelated sparse representation.
3.2 SPARSE CODING THROUGH LOCAL NEURAL INHIBITION 
The loss imposed by the SNI objective will only be zero when there is at most one active neuron
per example. This seems to be too harsh for complex tasks that need a richer representation. Thus,
we suggest to relax the objective by imposing a spatial weighting to the correlation penalty. In other
words, an active neuron penalizes mostly its close neighbours and this effect vanishes for neurons
further away. Instead of uniformly penalizing all the correlated neurons, we weight the correlation
penalty between two neurons with locations i and j using a Gaussian weighting. This gives
1 -ey =i" npn
Rsunt 
m
Published as a conference paper at ICLR 2019
As such, each active neuron inhibits its neighbours, introducing a locality in the network inspired
by biological neurons. While the notion of neighbouring neurons is not well established in a fully
connected network, our aim is to allow few neurons to be active and not only one, thus those few
activations dont have to be small to compensate for the penalty. o
is a hyper parameter representing
the scale at which neurons can affect each other. Note that this is somewhat more flexible than
decorrelating neurons in fixed groups as used in[Xiong et al.|( Our regularizer inhibits locally
the active neurons leading to a sparse coding through local neural inhibition.
3.3. NEURON IMPORTANCE FOR DISCOUNTING INHIBITION
Our regularizer is to be applied for each task in the learning sequence. In the case of tasks with
completely different input patterns, the active neurons of the previous tasks will not be activated given
the new tasks input patterns. However, when the new tasks are of similar or shared patterns, neurons
used for previous tasks will be active. In that case, our penalty would discourage other neurons from
being active and encourage the new task to adapt the already active neurons instead. This would
interfere with the previous tasks and could increase forgetting which is exactly what we want to
overcome. To avoid such interference, we add a weight factor taking into account the importance of
the neurons with respect to the previous tasks. To estimate the importance of the neurons, we use as a
measure the sensitivity of the loss at the end of the training to their changes. This is approximated by
the gradients of the loss w.r.t. the neurons outputs  evaluated at each
data point. To get an importance value, we then accumulate the absolute value of the gradients over
the given data points obtaining importance weight a; for neuron n;:
M
_i : _ WLY ms f
a= Mu
|gi
m=
where n" is the output of neuron n; for a given input example x,,,, and 6 are the parameters after
learning task n. This is in line with the estimation of the parameters importance in[Kirkpatrick etal]
but considering the derivation variables to be the neurons outputs instead of the parameters.
Instead of relying on the gradient of the loss, we can also use the gradient of the learned function, i.e.
the output layer, as done in{ATjundi et al.] for estimating the parameters importance. During the
early phases of this work, we experimented with both and observed a similar behaviour. For sake of
consistency and computational efficiency we utilize the gradient of the function when usin;
) as LLL method and the gradient of the loss when experimenting with EWC (
Then, we can weight our regularizer as follows:
64)
Rexnto 
J m
which can be read as: if an important neuron for a previous task is active given an input pattern from
the current task, it will not suppress the other neurons from being active neither be affected by other
active neurons. For all other active neurons, local inhibition is deployed. The final objective for
training is given in Eq. |2| setting Rggrz := Reivrp and Assi := Asunip. We refer to our full method
as Sparse coding through Local Neural Inhibition and Discounting .
4 EXPERIMENTS
In this section we study the role of standard regularization techniques with a focus on sparsity and
decorrelation of the representation in a sequential learning scenario. We first compare different
activation functions and regularization techniques, including our proposed SLNID, on permuted
MNIST (Sec. Then, we compare the top competing techniques and our proposed method in
the case of sequentially learning CIFAR-100 classes and Tiny Imagenet classes (Sec. 2p. Our
SLNID regularizer can be integrated in any importance weight-based lifelong learning approach such
as . Here we focus on Memory
, which is easy to integrate and experiment with and
. Finally, we show how our regularizer improves the state-of-the-art performance on a
sequence of object recognition tasks (Sec. [4.6}.
Published as a conference paper at ICLR 2019
a8 1) D4 Tale
Tl T2 3 v4 1S
Figure 2: Comparison of different regularization techniques on 5 permuted MNIST sequence. Representation
based regularizers are solid bars, bars with lines represent parameters regularizers, dotted bars represent activation
functions. Average test accuracy over all tasks is given in the legend. Representation based regularizers achieve
higher performance than other compared methods including parameters based regularizers. Our regularizer,
SLNID, performs the best on the last two tasks indicating that more capacity is left to learn these tasks.
SLNID:95.83
DeCov:95.68
L1-Rep:95.81
OrthReg:93.99
L1_Param:92.68
12_w0:92.91
Maxout:92.85
LWTA:92.85
ReLU:92.52
s 8 &
GERTTOEEE
Accuracy % after learning all tasks
4.1 AN IN-DEPTH COMPARISON OF REGULARIZERS AND ACTIVATION FUNCTIONS FOR
SELFLESS SEQUENTIAL LEARNING
We study possible regularization techniques that could lead to less interference between the different
tasks in a sequential learning scenario either by enforcing sparsity or decorrelation. Additionally, we
examine the use of activation functions that are inspired by lateral inhibition in biological neurons
that could be advantageous in sequential learning. MAS[Aljundi et al} is used in all cases as
LLL method.
Representation Based methods:
- L1-Rep: To promote representational sparsity, an L; penalty on the activations is used.
-Decov aims at reducing overfitting by decorrelating neuron activations. To
do so, it minimizes the Frobenius norm of the covariance matrix computed on the activations of the
current batch after subtracting the diagonal to avoid penalizing independent neuron activations.
Activation functions:
-Maxout network  utilizes the maxout activation function. For each group
of neurons, based on a fixed window size, only the maximum activation is forwarded to the next layer.
The activation function guarantees a minimum sparsity rate defined by the window size.
- LWTA similar idea to the Maxout network except that the non-maximum
activations are set to zero while maintaining their connections. In contrast to Maxout, LWTA keeps
the connections of the inactive neurons which can be occupied later once they are activated without
changing the previously active neuron connections.
- ReLU The rectifier activation function  used as a baseline here and
indicated in later experiments as NoReg as it represents the standard setting of sequential learning
on networks with ReLU. All the studied regularizers use ReLU as activation function.
Parameters based regularizers:
- OrthReg : Regularizing CNNs with locally constrained decorrelations. It
aims at decorrelating the feature detectors by minimizing the cosine of the angle between the weight
vectors resulting eventually in orthogonal weight vectors.
- L2-WD: Weight decay with Ly norm controls the complexity of the learned
function by minimizing the magnitude of the weights.
- L1-Param: L penalty on the parameters to encourage a solution with sparse parameters.
Dropout is not considered as its role contradicts our goal. While dropout can improve each task
performance and reduce overfitting, it acts as a model averaging technique. By randomly masking
neurons, dropout forces the different neurons to work independently. As such it encourages a
redundant representation. As shown by [2013a) the best network size for classifying
MNIST digits when using dropout was about 50% more than without it. Dropout steers the learning
of a task towards occupying a good portion of the network capacity, if not all of it, which contradicts
the sequential learning needs.
Experimental setup. We use the MNIST dataset  as a first task in a sequence of
5 tasks, where we randomly permute all the input pixels differently for tasks 2 to 5. The goal is to
classify MNIST digits from all the different permutations. The complete random permutation of the
pixels in each task requires the neural network to instantiate a new neural representation for each
pattern. A similar setup has been used by|Kirkpatrick et al.|; Goodfellow|
with different percentage of permutations or different number of tasks.
As a base network, we employ a multi layer perceptron with two hidden layers and a Softmax loss.
Published as a conference paper at ICLR 2019
mmm SLNID:63.3 mE DeCov:61.19 mmm L1-Rep:55.76 mam No-Reg55.31 mE SLNID:53.96 mE DeCov:52.47 mmm L1-Rep:52.57 mm No-Reg:49.56
$50
S
I, bas
240
mow a a
12 13. 1475
Accuracy % after learning all tasks
Figure 3: Comparison of different regularization techniques on a sequence of ten tasks from  Cifar split and
 Tiny ImageNet split. The legend shows average test accuracy over all tasks. Simple L1-norm regularizer
 doesnt help in such more complex tasks. Our regularizer SLNID achieves an improvement of 2%
over Decov and 4
8% compared to No-Reg.
We experiment with different number of neurons in the hidden layers {128,64}. For SLNID we
evaluate the effect of Asirp on the performance and the obtained sparsity in Figur In general, the
best Asinrp is the minimum value that maintains similar or better accuracy on the first task compared
to the unregularized case, and we suggest to use this as a rule-of-thumb to set Asinrp. For Ae, we
have used a high Ap value that ensures the least forgetting which allows us to test the effect on the
later tasks performance. Note that better average accuracies can be obtained with tuned Aq. Please
refer to Appendix[A]for hyperparameters and other details.
Results: Figure}2|presents the test accuracy on each task at the end of the sequence, achieved by the
different regularizers and activation functions on the network with hidden layer of size 128. Results
on a network with hidden layer size 64 are shown in the Appendix [B] Clearly, in all the different
tasks, the representational regularizers show a superior performance to the other studied techniques.
For the regularizers applied to the parameters, L2WD and L1Param do not exhibit a clear trend
and do not systematically show an improvement over the use of the different activation functions only.
While OrthReg shows a consistently good performance, it is lower than what can be achieved by
the representational regularizers. It is worth noting the L1Rep yields superior performance over
L1Param. This observation is consistent across different sizes of the hidden layers (in Appendix [Bp
and shows the advantage of encouraging sparsity in the activations compared to that in the parameters.
Regarding the activation functions, Maxout and LWTA achieve a slightly higher performance than
ReLU. We did not observe a significant difference between the two activation functions. However,
the improvement over ReLU is only moderate and does not justify the use of a fixed window size
and special architecture design. Our proposed regularizer SLNID achieves high if not the highest
performance in all the tasks and succeeds in having a stable performance. This indicates the ability of
SLNID to direct the learning process towards using minimum amount of neurons and hence more
flexibility for upcoming tasks.
Representation sparsity & important parameter sparsity.
aww = 0.02
aeeoi.a4
Dau = 0.005
pecans
Daag
0.002
Here we want to examine the effect of our regularizer on the
percentage of parameters that are utilized after each task and
hence the capacity left for the later tasks. On the network with
hidden layer size 128, we compute the percentage of parameters
with Q, < 10~?, with Q,, see Appendix [A] the importance
weight multiplier estimated and accumulated over tasks. Those
parameters can be seen as unimportant and "free" for later
tasks. Figure[4] top) shows the percentage of the unimportant
 parameters in the first layer after each task for different
Asinitp Values along with the achieved average test accuracy ea
at the end of the sequence. It is clear that the larger Astytp, 44
i.e., the more neural inhibition, the smaller the percentage of 30)
important parameters. Apart from the highest Aszyrp where aa
tasks couldnt reach their top performance due to too strong .
inhibition, improvement over the NoReg is always observed. wee
The optimal value for lambda seems to be the one that remains Figure 4: On the 5 permuted MNIST
close to the optimal performance on the current task, while Sequence, hidden layer=128, Top: per-
utilizing the minimum capacity feasible. Next, we compute _&Mage of unused parameters in the Ist
the average activation per neuron, in the first layer, over all ayer using different Asiv:0; Bottom:
x : histogram of neural activations on the
the examples and plot the corresponding histogram for SLNID, goo tack
DeCov, L1-Rep, L1-Param and No-Reg in Figure[4{bot- ,
tom) at their setting that yielded the results shown in Figure] SLNID has a peak at zero indicating
representation sparsity while the other methods values are spread along the line. This seems to hint at
aces.
Asaig = 9.0005
aces 34
aap = 0,002,
acces
No Reg
* gcci92 52
Parmeters Sparsity %
Published as a conference paper at ICLR 2019
the effectiveness of our approach SLNID in learning a sparse yet powerful representation and in turn
in a minimal interference between tasks.
| Permuted mnist Cifar | Method Avg-ace |
| h-layer dim. 128 | 64 256 | 128 Finetune 32.67
[No-Reg 92.67 [90.72 [55.06 [55.3 49.49
SNI 95.79 | 94.89 | 55.30] 55.75 50.29
SNID 95.90 | 93.82 | 61.00 | 60.90 43.4
SLNI 95.95| 94.87 | 56.06 | 55.79 50.49
SLNID 95.83 | 93.89 | 63.30 | 61.16 50.00
[ Multi-Task Joint Training* [97.30 [96.80 _ [70.99 [71.95 re
Table 1: SLNID ablation. Average test accuracy per SLNTD-fe randomly initialized  | 54.50
task after training the last task in %. * denotes that Multi- . .
Task Joint Training violates the LLL scenario as it has Table 2: 8 tasks object recognition se-
access to all tasks at once and thus can be seen as an quence. Average test accuracy per task
upper bound. after training the last task in %.
4.2. 10 TASK SEQUENCES ON CIFAR-100 AND TINY IMAGENET
While the previous section focused on learning a sequence of tasks with completely different input
patterns and same objective, we now study the case of learning different categories of one dataset.
For this we split the CIFAR-100 and the Tiny Eni geDiet  dataset into ten tasks,
respectively. We have 10 and 20 categories per task for AR- and Tiny ImagNet, respectively.
Further details about the experimental setup can be found in appendix [A]
We compare the top competing methods from the previous experiments, L1Rep, DeCov and our
SLNID, and No-Reg as a baseline, ReLU in previous experiment. Similarly, MAS [ATjundi et al]
is used in all cases as LLL method. Figures[3[a) and[3{b) show the performance on each of the
ten tasks at the end of the sequence. For both datasets, we observe that our SLNID performs overall
best. L1-Rep and DeCov continue to improve over the non regularized case NoReg. These results
confirm our proposal on the importance of sparsity and decorrelation in sequential learning.
4.3. SLNID WITH EWC 2016)
We have shown that our proposed regularizer SLNID exhibits stable and superior performance on
the different tested networks when using MAS as importance weight preservation method. To prove
the effectiveness of our regularizer regardless of the used importance weight based method, we
have tested SLNID on the 5 tasks permuted MNIST sequence in combination with Elastic Weight
Consolidation (EWwC|Kirkpatrick et al.| and obtained a boost in the average performance at
the end of the learned sequence equal to 3.1% on the network with hidden layer size 128 and a
boost of 2.8% with hidden layer size 64. Detailed accuracies are shown in Appendix|B} It is worth
noting that with both MAS and EWC our SLNID was able obtain better accuracy using a network with
a 64-dimensional hidden size than when training without regularization NoReg on a network of
double that size , indicating that SLNID allows to use neurons much more efficiently.
4.4 ABLATION STUDY
Our method can be seen as composed of three components: the neural inhibition, the locality relax-
ation and the neuron importance integration. To study how these components perform individually,
Table [I] reports the average accuracy at the end of the Cifar 100 and permuted MNIST sequences
for each variant, namely, SNID without neuron importance , SNID, SLNID without neuron
importance  in addition to our full SLNID regularizer. As we explained in Section} when
tasks have completely different input patterns, the neurons that were activated on the previous task
examples will not fire for new task samples and exclusion of important neurons is not mandatory.
However, when sharing is present between the different tasks, a term to prevent SLNID from causing
any interference is required. This is manifested in the reported results: for permuted MNIST, all
the variants work nicely alone, as a result of the simplicity and the disjoint nature of this sequence.
However, in the Cifar 100 sequence, the integration of the neuron importance in the SNID and SLNID
Published as a conference paper at ICLR 2019
regularizers exclude important neurons from the inhibition, resulting in a clearly better performance.
The locality in SLNID improves the performance in the Cifar sequence, which suggests that a richer
representation is needed and multiple active neurons should be tolerated.
4.5 SEQUENTIAL LEARNING WITHOUT HARD TASK BOUNDARIES
In the previous experiments, we considered the standard task based scenario as in
Zenke et al.| 2017} Aljundi et al. 2017] Serra et al. 2018), where at each time step we receive a task
along with its training data and a new classification layer is initiated for the new task, if needed. Here,
we are interested in a more realistic scenario where the data distribution shifts gradually without hard
task boundaries.
To test this setting, we use the Cifar 100 dataset. Instead
of considering a set of 10 disjoint tasks each composed of [Method Avg.ace -tasks models
10 classes, as in the previous experiment (Sec. , we now poe MAS oa
: : : a 9 SLNI w/o MAS - Io
start by sampling with high probability  from the first | soutpwyo mas | 73.03%
10 classes and with low probability  from the rest of the | No-Reg 66.88%
classes. We train the network (same architecture as in Sec a ees
33%
for a few epochs and then change the sampling probabilities
Method Avg.acc-last model |
to be high  for classes 11
20 and low  for the [\o-Regu7o WAS T-65.15%
remaining classes. This process is repeated until sampling | SLNIw/o mas 63.54%
with high probability from the last 10 classes and low from the
MAS oor
rest. We use one shared classification layer throughout and esti- | sit 64.50%
mate the importance weights and the neurons importance after _SLNID 70.94%
each training step . .
We consider 6 variants: our SLNID, the ablations SLNI and Table 3: No tasks boundaries test case
without regularizer No-Reg, as in Section[4.4] as well each 07 Cifar 100. Top block, avg. acc on
of these three trained without the MAS importance weight cach eroup oF classes Using each group
: model. Bottom block, avg. acc. on each
regularizer of[Aljundi et al.|, denoted as w/o MAS. Ta- group at the end of the training.
blepresents the accuracy averaged over the ten groups of ten
classes, using each group model (i.e. the model trained when this group was sampled with high prob-
ability) in the top block and the average accuracy on each of the ten groups at the end of the training
 SLNID improves the performance
considerably  In this scenario
without hard task boundaries there is less forgetting than in the scenario with hard task boundaries
studied in Section|4.2]for Cifar (difference between rows in top block to corresponding rows in middle
block). As a result, the improvement obtained by deploying the importance weight regularizer is
moderate: at 70.75%, SLNID w/o MAS is already better than NoReg reaching 66.33%. 3) While
SLNI without MAS improves the individual models performance , it
fails to improve the overall performance at the end of the sequence , as
important neurons are not excluded from the penalty and hence they are changed or inhibited leading
to tasks interference and performance deterioration.
4.6 COMPARISON WITH THE STATE OF THE ART
To compare our proposed approach with the different state-of-the-art sequential learning methods,
we use a sequence of 8 different object recognition tasks, introduced in . The
sequence starts from AlexNet  pretrained on ImageNet (Russakovsky|
. More details are in
Appendix|A.4] We compare against the following: Learning without Forgetting 
 and
sequential finetuning  alone, i.e.
our No-Reg before. Compared methods were run with the exact same setup as in
. For our regularizer, we disable dropout, since dropout encourages redundant activations
which contradicts our regularizers role. Also, since the network is pretrained, the locality introduced
in SLNID may conflict with the already pretrained activations. For this reason, we also test SLNID
with randomly initialized fully connected layers. Our regularizer is applied with MAS as a sequential
learning method. Table 2|reports the average test accuracy at the end of the sequence achieved by
each method. SLNID improves even when starting from a pretrained network and disabling dropout.
Published as a conference paper at ICLR 2019
Surprisingly, even with randomly initialized fully connected layers, SLNID improves 1.8% over the
state of the art using a fully pretrained network.
5 CONCLUSION
In this paper we study the problem of sequential learning using a network with fixed capacity
a
prerequisite for a scalable and computationally efficient solution. A key insight of our approach is
that in the context of sequential learning (as opposed to other contexts where sparsity is imposed,
such as network compression or avoiding overfitting), sparsity should be imposed at the level of the
representation rather than at the level of the network parameters. Inspired by lateral inhibition in
the mammalian brain, we impose sparsity by means of a new regularizer that decorrelates nearby
active neurons. We integrate this in a model which learns selflessly a new task by leaving capacity
for future tasks and at the same time avoids forgetting previous tasks by taking into account neurons
importance.
Acknowledgment: The first authors PhD is funded by an FWO scholarship.

