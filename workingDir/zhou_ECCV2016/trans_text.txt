Last few years have witnessed the success of deep convolutional neural networks
(CNN) in many computer vision applications. One important reason is the emer-
gence of large annotated datasets and the development of high-performance
computing hardware facilitating the training of high capacity CNNs with an
exceptionally large number of parameters.

When defining the structure of a network, large networks are often preferred,
and strong regularizers [i] tend to be applied to give the network as much dis-
criminative power as possible. As such, the state-of-the-art CNNs nowadays con-
tain hundreds of millions of parameters [2]. Most of these parameters come from
one or two layers that host a large number of neurons. Take AlexNet [3] as
an example. The first and the second fully connected layers, which have 4096
neurons each, contain around 89% of all the parameters. Having to use such a
large number of parameters leads to high memory requirements when mapping
and running the network on computational platforms. Consequently, using deep
CNNs obligates significant hardware resources, which hinders their practicability
for mobile computing devices that have limited memory specifications.

Several recent studies have focused on reducing the size of the network con-
cluding that there is substantial redundancy in the number of parameters of
a CNN. For instance, [4,5] represent the filters in a CNN using a linear com-
bination of basis filters. However, these methods can only apply to an already
trained network. Other works have investigated directly reducing the number
of parameters in a CNN by using sparse filters instead of its original full-size
filters. To the best of our knowledge, existing deep learning toolboxes [(,7,8] do
not support sparse data structures yet. Therefore, special structures need to be
implemented. Moreover, FFT is shown to be very efficient to compute convo-
lutions on GPUs [9]. However, a sparse matrix is usually no longer sparse in
the Fourier domain, which limits the applicability (ie., reduction of memory
footprint) of these methods.

To address the shortcomings of the aforementioned works, we propose an
efficient method to decimate the number of neurons. Our approach has four key
advantages: (1) Neurons are assessed and reduced during the training phase.
Therefore, no pre-trained network is required. (2) The reduction of parameters
do not rely on the sparsity of neurons; thus, our method can be directly included
in popular deep learning toolboxes. (3) The number of filters in Fourier domain is
proportional to the number of neurons in spatial domain. Therefore, our method
can also reduce the number of parameters in the Fourier domain. (4) Reducing
the number of neurons of a layer directly condenses the data dimensionality
when the output of an internal layer is utilized as image features [10,11].

Our method consists of imposing sparse constraints on the neurons of a CNN
in the objective function during the training process. Furthermore, to minimize
the extra computational cost of adding these constraints, we solve the sparse con-
strained optimization problem using forward-backward splitting method [12,13].
The main advantage of forward-backward splitting is the bypass of the sparse
constraint evaluations during the standard back-propagation step, making the
implementation very practical. We also investigate the importance of rectified
linear units in sparse constrained CNNs. Our experiments show that rectified
Less is More: Towards Compact CNNs 3

linear units help to reduce the number of neurons leading to even more compact
networks.

We conduct a comprehensive set of experiments to validate our method using
four well-known models (ie., LeNet, CIFAR-10 quick, AlexNet and VGG) on
three public datasets including ImageNet. Table | summarizes a representative
set of our results when constraints are applied to the first fully connected layer
of each model, which has the largest number of neurons and parameters. As
shown, even with a large portion of neurons removed and, therefore, a significant
reduction in the memory footprint of the original model; the resulting networks
still perform as good as the original ones. These results are also convenient for
deployment in mobile platforms where computational resources are relevant. For
instance, using single float type (4 bytes) to store the network, the amount of
memory saved for AlexNet in Table 1 is 152 MB.

To reiterate, the contribution of this paper is threefold. First, we remove
neurons of a CNN during the training stage. Second, we analyze the importance
of rectified linear units for sparse constraints. Finally, our experimental results
on four well-known CNN architectures demonstrate a significant reduction in
the number of neurons and the memory footprint of the testing phase without
affecting the classification accuracy.

3 Sparse Constrained Convolutional Neural Networks

Notation: In the following discussion, if not otherwise specified, |-| is the ¢;
norm, ||x|| = \/}°, 2? is the ¢2 norm for a vector (Frobenius norm for a matrix).

We use W and b to denote all the parameters in filters and bias terms in a CNN,
respectively. w; and b; represent the filter and bias parameters in the /-th layer.
w; is a tensor whose size is w x h x m x n, where w and h are the width and the
height of a 2D filter, m represents the number of channels for the input feature
and n is the number of channels for the output feature. b; is a n dimensional
vector, i.e. each output feature of this layer has one bias term. w); represents a
wxhxm filter that creates the j-th channel of the output feature for layer 1, bj;
is its corresponding bias term. w); and bj; together form a neuron. We use Ww,
Less is More: Towards Compact CNNs 5

w; and Ww); to represent the augmented filters (they contain the corresponding
bias term).

3.1 Training a Sparse Constrained CNN

Let {X,Y*} be the training samples and corresponding ground-truth label.
Then, a CNN can be represented as a function Y = f(W,X), where Y is the
output of the network. W is learned through minimizing an objective function:

min v(f(W, X), ¥"). (1)
Ww

We use u(W) to represent the objective function for simplicity. The objective
function u(W) is usually defined as the average cross entropy of the ground
truth labels with respect to the output of the network for each training image.
Equation (1) is usually solved using a gradient descend based method such as
back-propagation [28].

Our goal is adding sparse constraints on the neurons of a CNN. Therefore,
the optimization problem of (1) can be written as:

min y(W) + g(W), (2)
Ww

where g(W) represents the set of constraints added to W. Given this new op-
timization problem, the k-th iteration of a standard back-propagation can be
defined as:

Oy(W) _ 29(W)

We WE ge Wewint T og hawinn

(3)

where WF represents the parameters learned at k-th iteration and 7 is the learn-

Baw)

ing rate. Based on (3), anew term |vwv—we-1 must be added to the gradient

of each constrained layer during back. propagation. In those cases where g(W)
is non-differentiable at some points, sub-gradient methods of g(W) are usually
needed. However, these methods have three main problems. First, iterates of the
sub-gradient at the points of non-differentiability hardly ever occur [12]. Second,
sub-gradient methods usually cannot generate an accurate sparse solution [29].
Finally, sub-gradients of some sparse constraints are difficult to choose due to
their complex form or they may not be unique [13]. To avoid these problems, and
in particular with 1, constrained optimization problems, [30] and [26] proposed
to use proximal mapping. Following their idea, in the next section, we apply
proximal operator to our problem.

3.2 Forward-Backward Splitting

Our proposal to solve the problem (2) and therefore, train a constrained CNN,
consists of using forward-backward splitting algorithm [12,13]. Forward-backward
6 Hao Zhou, Jose M. Alvarez and Fatih Porikli

splitting provides a way to solve non-differentiable and constrained large-scale
optimization problem of the generic form:

min f(z) +h(2), (4)

where z € RY, f(z) is differentiable and h(z) is an arbitrary convex function
[12,13]. The algorithm consists of two stages: First, a forward gradient descent
on f(z). Then, a backward gradient step evaluating the proximal operator of
h(a). Using this algorithm has two main advantages. First, it is usually easy to
estimate the proximal operator of h(z) or even having a closed form solution.
Second, backward analysis has an important effect on the convergence of the
method when f(z) is convex [13].

Though there is no guarantee about convergence when f(z) is non-convex,
forward-backward splitting method usually works quite well for non-convex op-
timization problems [13]. By treating y(W) in Equation (2) as f(z), the forward
gradient descent can be computed exactly as the standard back-propagation al-
gorithm in training CNNs. As a result, using forward-backward splitting method
to solve sparse constrained CNNs has two steps in one iteration. , leontin Z
shows how to apply this method to optimize Equation (2), where r* is the learn-
ing rate of forward step at k-th iteration.


In practice, we define one step in line 2 of Algorithm 1 as one epoch instead
of one iteration of the stochastic gradient descent algorithm. There are two main
reasons for this. First, to minimize the computational training overhead of the
algorithm as we need to estimate fewer proximal operators of g(W). Second, the
gradient of u(W) at each iteration is an approximation to the exact gradient
which is noisy [30]. Computing the gradient after certain number of iterations
would make the learned parameters more stable [29].

4 Sparse Constraints

Our goal is removing neurons w);, each of which is a tensor. To this end, we
consider two sparse constraints for g(W) in Equation (2): tensor low rank con-
straints [31] and group sparsity [32].

4.1 Tensor Low Rank Constraints

Although the low-rank constraints for 2D matrices and their approximations
have been extensively studied, as far as we know, there are few works considering
Less is More: Towards Compact CNNs t

the low-rank constraints for higher dimensional tensors. In [31], the authors
proposed to minimize the average rank of different unfolding of a tensor matrix.
To relax the problem to convex, they proposed to approximate the average rank
using the average of trace norms, which is called tensor trace norm, for different
unfolding [31]. We use this formulation as our tensor low rank constraints.

The tensor trace norm of a neuron Wy; is || Willer = 4 jy | W(|lers Where
n is the order of tensor W);, and W7;(;) is the result of unfolding the tensor wi;
along the i-th mode. Under this definition, function g(W) can be defined as:

; Le
g(W) =r > = do llWscaller (5)

G.DEQ” i=l

where (2 is a set containing all the neurons to be constrained and ) is the weight
for the sparse constraint.
As a result, the backward step in the forward-backward splitting is given by:

1 1 2
ak * a ak a kx 2 a
= aremin — y ay| leo = y ye 12 6
Wy) = arg oa re fy ca [ler + Bran 24 IM i@ — Will (6)

This problem can be solved using the Low Rank Tensor Completion (LRTC)
algorithm proposed in [31].

4.2 Group Sparse Constraints

These are defined as 12; regularizer. Applying 12; to our objective function, we
have:

g(W) =r S> [lvl], (7)

GDER

where A and §2 are the same as in Section 4.1. ||.|| is defined in Section 3.
According to [32], backward step in forward backward splitting method at k-th
iteration now becomes:
whe
wh, = max{||wh*|| — 7A, 0} 2 (8)

Iw?

 

where T is the learning rate and wie is the optimized neuron from the forward
step in forward backward splitting.

5 Importance of Rectified Linear Units in Sparse
Constrained CNNs

Convolutional layers in a CNN are usually followed by a nonlinear activation
function. Rectified Linear Units (ReLU) [3] has been heavily used in CNNs for

computer vision tasks. Besides the advantages of ReLU discussed in [33,54,35], we
show that Ww); = 0 is a local minimum in sparse constrained CNNs, of which the
non-linear function is ReLU. This can explain our findings that ReLU can help
removing more neurons and inspires us to set momentum to 0 for neurons which
reach their sparse local minimum during training as discussed in Section 6.1.
ReLU function was defined as ReLU (x) = max(0, x) which is non-differentiable

at 0. This brings us difficulty to analyze the local minimum of sparse constrained
CNNs. Based on the observation that, in practical implementations, the gradi-
ent of ReLU function at 0 is set to 0 [6,7,8]. We consider the following practical
definition of ReLU:

x ife>e
0 ifa<e.

ReLU(x) = { (9)

Where € is chosen such that for any real number x that a computer can repre-
sent, if x > 0, then a > e; if  < 0, then x < e. The non-differentiable point
of ReLU function is now at € which will never appear in practice. Under this
definition, ReLU function is differentiable and continuous at point 0, the gradi-
ent of ReLU(a) at 0 is now 0. As a result, this practical definition of ReLU is
consistent with the implementations of ReLU function in practice [(,7,8].

By fixing all other neurons, it is not difficult to show that a particular neuron
Wij = 0 lies in a flat region of ~(w);) under the above definition of ReLU.
Moreover, g(Wi;) contains a sparse constrains, so Wj; = 0 is the local minimum
of the objective function {(W1;) + g(Wij).?

The fact that w,; = 0 is a local minimum for sparse constrained CNNs
using ReLU as nonlinear activation function can explain the improvement in
the number of neurons removed as discussed in Section 6.1. Importantly, we
find that using momentum during the optimization may push a zero neuron
away from being 0 since momentum memorizes the gradient of this neuron in
previous steps. As a consequence, using momentum would lead to more non-zero
neurons without performance improvement. In practice, once a neuron reaches
0, we set its momentum to zero, forcing the neuron to maintain its value. As
we will demonstrate in Section 6.1 this results in more zero neurons without
affecting the performance.

6 Experiments

We test our method on four well-known convolutional neural networks on three
well-known datasets: LeNet on MNIST [28], CIFAR10-quick [36] on CIFAR-10
[87] and AlexNet [3] and VGG [2] on ILSVRC-2012 [38]. We use LeNet and
CIFAR10-quick provided by Matconvnet [8] and AlexNet and VGG provided by
[39] to carry out all our experiments.

All the structures of the networks are provided by Matconvnet [8] or [39],
the only change we made is to add ReLU function after each convolutional layer

Weight for sparse constraints

(a) (b)

*" Weight for sparse constraints

Fig. 1. (a) Percentage of nonzero neurons on the second convolutional layer for LeNet
under different weights for sparse constraints with and without adding ReLU layer. (b)
Corresponding top 1 validation error rate. Baseline in (b) shows the top 1 validation
error rate without adding any sparse constraints. Error bars represent the standard
deviation over the four experiments.

in LeNet, which we will discuss in detail later. For all our experiments, data
augmentation and pre-processing strategies are provided by [8] and [39].

6.1 LeNet on MNIST

MNIST is a well-known dataset for machine learning and computer vision. It
contains 60,000 handwritten digits for training and 10, 000 for testing. All these
digits are images of 28 x 28 with a single channel. All the data will be subtracted
by the mean as suggested in [8]. The average top 1 validation error rate of LeNet
on MNIST adding ReLU layer is 0.73%. As training a LeNet on MNIST is fast,
we use this experiment as a sanity check. Results are computed as the average
over four runs of each experiment using different random seeds.

ReLU helps removing more neurons: The LeNet structure provided by
Matconvnet has two convolutional layers and two fully connected layers. The
two convolutional layers are followed by a max pooling layer respectively. Under
this structure, the sparse solution may not be a local minimum. Based on the
discussion in Section 5, we add a ReLU layer after each of these two convolutional
layers so that Ww); = 0 is a local minimum. We compare these two structures by
using different weights for sparse constraints added to the second convolutional
layer and show the results in Figure 1.

As shown in Figure 1, adding ReLU improves the performance of LeNet no
matter whether we add sparse constraints or not. Comparing these two struc-
tures, adding ReLU layer always leads to more zero neurons compared with the
original structure. What is more interesting is that, with ReLU layer, the top
1 validation error rate of LeNet with sparse constraints is more close to, if not
better than, the performance without sparse constraints. This may be explained
by the fact the W,; = 0 is a local minimum of the structure with ReLU and this
local minimum is usually a good one. In the following discussion, when LeNet is
10 Hao Zhou, Jose M. Alvarez and Fatih Porikli

 

 

 

 

 

 
   
     

 

 

 

 

 

 

 

   
   
   

  
   

 

 

 

 

 

 

 

 

 

 

 

= tensor bw rank momentum =0
ge Loow| group sparse, momentum =0 ff _
& “& tensor lowrank, moment Sua
ra :[--group sparse, momentum ==0 [| ©
2
e¢ g
8 go
§ 8
Ba 3
Z| su
< a
Ex &
5 Baal
‘Weight for sparse constraints “Weight for sparse constraints
(a) ; (b)
= tensor bw rank, momentum ae tensor loa rank moment
| 3] group sparse, momenta eal A 9:0UP sparse, momentum
& "Ye tensor ionrank, moment Ya tensor lowranic moment:
 *|L-© oun sparse, momentum ~= & |= grou sparse, momentum
8 go —
| Bo
8 a
R 5 gol
& 8
24 Bae
2, 8
c A od
Eo &
é F aa

 

Weight for sparse constraints

(c) (a)

“Weight for sparse constraints

Fig. 2. Comparison of the proposed results with setting momentum to be zero. (a) an
(c) show the percentage of non zero neurons for second convolutional layer and first
fully connected layer under different weights for sparse constraints respectively. (b) and
(d) show their corresponding top 1 validation error rate.

mentioned, we mean LeNet with ReLU function after each of its convolutiona
layers.

Momentum for sparse local minimum: Momentum plays a significant
role in training CNNs. It can be treated as a memory of the gradient computec
in previous iterations and has been proved to accelerate the convergence of SGD.
However, this memory of gradient effect may push a neuron in its sparse loca
minimum away, leading to results with more non-zero neurons with no improve-
ment in performance. To avoid this problem we can directly set the momentum
of the neurons to be zero for those reached sparse local minimum.

In Figure 2, we compare the number of non-zero neurons with and withou
setting the momentum to be zero for LeNet on MNIST dataset. We add the
sparse constraints on the second convolutional layer and the first fully connecte:
layer since they have most of the filters. As shown in Figure 2, the performance
does not drop when momentum is set to be zero for local sparse minimum.
Additionally, we sporadically achieve better performance. Moreover, for firs
fully connected layer, setting momentum to be zero leads to results with more
zero neurons under large sparse weights. *

 

3 For a clear comparison, we only show results under large weights for the first fully
connected layer in the figure.
Less is More: Towards Compact CNNs 11

 

   
    

 

‘Proposed result on fea a Towrank constainls Tor conv
2

 

 

 

 

 

 

= ata-free resutonfc3 zal] group sparse constiants for conv3|

® 5] —=no sparse constraints |. don BX _ || Ae towrank constraints tor fea

g & || So tfoup sparse constant trict
2 || without adding sparse constraints

p a 8 xf -

5 c

e B,

a a o

3g. ag Ea

: g

A o 3

a? Ba

2 a

¢ a:

2 g

s 8

3 + eR)

 

 

 

 

 

Remaining non zero neurons (%)

(a) (b)

Remaining non zero neurons (%)

Fig. 3. a) MNIST Comparison between of single layer results of the proposed method
and the data-free method presented in [|] using LeNet on MNIST. b) CIFAR-10 Top
1 validation error rate versus percentage of remaining non zero neurons on conv3 and
fce4 using CIFAR10-quick.

From Figure 1 and Figure 2, we find that the top-1 error rates of using
tensor low-rank constraints are a little better than using group sparse constraints.
Group sparse constraints, however, can give more zero neurons which can result
in more compact networks. These differences are very small, so we conclude
that both of these methods can help removing neurons without hurting the
performance of the network. In the following discussion, only results from group
sparse constraints are shown when necessary.

Compression for LeNet: Figure 3 (a) shows the comparison of the pro-
posed method with [1]. Since [1] only adds sparse constraints on first fully con-
nected layer of LeNet (fc3), we only show our results using group sparse con-
straints on fc3. The compression rate and top 1 validation error are averaged over
4 results with different random seeds. Since the two networks may be trained
differently, we show the relative top 1 validation error rate, which is defined as
top 1 validation error rate of the proposed method minus that without sparse
constraints. A smaller value means better performance. As shown in this figure,
our method outperforms the one proposed in [1] in both compression and accu-
racy. Please, note that [1] predicts the cut-off number of neurons for fc3 to be
440 and a drop of performance of 1.07%. The proposed method, on the contrary,
can remove 489 neurons while maintaining the performance of the original one.

To improve the compression rate, we add group sparse constraints to the
layers containing most of the parameters in LeNet: the second convolutional
layer (conv2) and the first fully connected layer (fc3). We compare two strategies.
First, adding sparse constraints in a layer by layer fashion and second, jointly
constraining both layers. Empirically we found that the first strategy performs
better. Thus, we first add sparse constraints on fe3 and, after the number of zero-
neurons in this layer is stable, we add sparse constraints on conv2. We report the
results of using group sparse constraints. The weight used for fc3 is set to 100,
and the weights for conv2 are set to 60,80, 100. Table 3 summarizes the average
results over the four runs of the experiment. As shown, our method not only
reduces significantly the number of neurons, which leads to significant reduction
12 Hao Zhou, Jose M. Alvarez and Fatih Porikli

 

 

Table 2. Results of adding group sparse constraints on three layers.
a Non-zero Neurons|Number of |Top-1 error (%)
conv1]conv2|fc3}conv1|conv2| fe3 |Parameters
100 | 80 |90| 7 23 | 20 11820 0.72
100 | 120 |80| 7 13 | 21 7079 0.76
120 | 120 |90| 6 14 | 20 6980 0.81

 

 

 

 

 

 

 

 

in the number of parameters in these two layers but also compresses the total
number of parameters of the network for more than 90%, leading to a memory
footprint reduction larger than 1 MB.

We further try to add group sparse constraints on all three layers of LeNet
to check whether our method can work on more layers. To introduce more re-
dundancy on conv1 and conv2, we initialize the number of non zero neurons for
conv1 and conv2 to be 100. Similar to adding sparse constraints on two layers, we
add sparse constraints layer by layer. We show some of our results in Table 2. To
compare, the best compression result of adding sparse constraints on conv2 and
fc3 leads to a model with 13062 parameters (third row of LeNet in Table 2). We
find that by adding sparse constraints on three layers, a more compact network
can be achieved though we initialize the network with more neurons.

6.2 CIFAR-10 quick on CIFAR-10

CIFAR-10 [37] is a database consisting of 50,000 training and 10,000 testing
RGB images with a resolution of 32 x 32 pixels split into 10 classes. As suggested
in [8], data is standardized using zero-mean unit length normalization followed by
a whitening process. Furthermore, we use random flips as data augmentation. For
a fair comparison, we train the original network, CIFAR10-quick, without any
sparse constraints using the same training set-up and achieve a top 1 validation
error rate of 24.75%.

Since the third convolutional layer (conv3) and the first fully connected layer
(fc4) contain most of the parameters, we add sparse constraints on these two
layers independently. Figure 3 (b) shows the top 1 validation error rate versus the
percentage of remaining non-zero neurons. As shown, 20% and 70% of neurons
for conv3 and fc4 can be removed without a noticeable drop in performance.

To obtain the best compression results, we jointly constraint conv3 and fc4
as we did with LeNet. To this end, we first add the constraints on fc4, and then,
we include the same ones on conv3. We run the experiment for more epochs com-
pared to the default values in Matconvnet [8]. For a fair comparison, we train the
baseline (i.e., CIFAR-10 quick without sparse constraints) for the same number
of epochs. As a result, we obtain a top 1 validation error of 22.33%. We used
this result to compute relative top 1 validation error rate. The weight of group
sparse constraints for fc4 is fixed to 280 while the three different weights for
conv3 are 220, 240, 280. A summary of results is listed in the second part of Ta-
ble 3. Through this experiment, it can be seen that even for this simple network,
Less is More: Towards Compact CNNs 13

 

 

 

 

[tet eons
|S proposes resutonte?

B ‘Lp datoctee esunon eo
5 | Stetson cme

5.

e

2

3 a

8

|

8 *
2° + a
3 af Si.

 

 

 

 

Remaining non zero neurons (%)

Fig. 4. ImageNet. Comparison between single layer results on fe6 and fc7 and the
data-free method in [1] using AlexNet on ImageNet. Data-free results are from [1].

we can remove a large number of neurons which leads to a great compression in
the number of parameters. Considering that there are only 64 neurons on each
of these two layers, the compression results are significant.

6.3 AlexNet and VGG on ImageNet

ImageNet [10] is a dataset with over 15 million labeled images split into 22,000
categories. AlexNet ['3] was proposed to be trained on ILSVRC-2012 [38] which
is a subset of ImageNet with 1.2 million training images and 50,000 validation
images. We use the implementation of AlexNet and VGG-13 provided by [39] in
order to test ImageNet on our cluster. Random flipping is applied to augment
the data. Quantitative results are reported using a single crop in the center of
the image. For comparison, we consider the network trained without adding any
sparse constraints. The top 1 validation error for this baseline is 45.57% and
37.50% for AlexNet and VGG-13 respectively. For the rest of experiments we
only report results using group sparse constraints.

Figure 4 shows the top 1 validation error rate versus the percentage of non-
zero neurons for AlexNet. Group sparse constraints are added on first and second
fully connected layers (fc6 and fc7) independently. Results from [1] are copied
from their paper and shown in this figure for comparison. Similar to Section 6.1,
we show the relative top 1 validation error rates due to the training of two
methods may be different. This figure clearly shows that compared with [1], the
proposed method can remove a large number of neurons without decreasing the
performance. For instance, the best compression results of the proposed method
can eliminate 76.76% of all the parameters of AlexNet with a negligible drop
in performance (0.57% in top 1 validation error). The best performance model
in [1], on the other hand, can only remove 34.89% of the parameters with top
1 validation error rate decreased by 2.24%. A representative set of compression
results obtained using sparse constraints on two layers is shown in the third part
of Table 3.

We test the proposed method on VGG-13 on the first fully connected layer
as it contains most of the parameters of the network. Table 4 summarizes the
outcomes of the experiment for different group sparsity weights. As shown, for
14 Hao Zhou, Jose M. Alvarez and Fatih Porikli

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Table 3. Results of adding group sparse constraints on two layers. The best compres-
sion results within 1% decrease in top 1 error rate is shown in bold.
T Neurons pruned(%) |Top-1 error (%)| parameter memory
conv2| fc3 |conv2 fc3 absolute|relative|reduction (%)|reduced (MB)
LeNet | 60 |100] 45.5 97.75 0.73 0.00 95.35 1.57
80 |100] 56.5 97.75 0.77 0.04 96.31 1.58
100 |100| 63.0 97.75 0.76 0.03 96.79 1.59
T Neurons pruned (%)]Top 1 error (%)] parameters memory
conv3| fc4 |conv3 fc absolute|relative] reduction(%) |reduced (KB)
cifarl0-| 220 |280) 31.25 70.31 22.21 | -0.12 47.17 268.24
quick | 240 /280) 46.88 71.86 22.73 0.4 55.15 313.62
280 |280) 54.69 70.31 23.78 | 1.45 58.56 333.01
T Neurons pruned (%)]Top 1 error (%)] parameters memory
fc6 |fc7| fc6 fc7 absolute|relative|reduction (%)| reduced(MB)
AlexNet] 40 | 35 |48.46 56.49 44.58 | -0.98 55.15 128.26
45 | 30] 77.05 60.21 46.14 | 0.57 76.76 178.52
45 | 35 | 73.39 65.80 45.88 | 0.31 74.88 174.14
Table 4. Some compression results of proposed method on fel for vgg-B. Neuron:
compression of neurons in the fel. Parameter: compression of total parameters.
layer| 7] compression % memory top 1 error (%)
neurons|parameters|reduced (MB)|absolute|relative
fel |5| 39.04 35.08 178.02 38.30 | 0.80
fcl [10] 49.27 44.28 224.67 38.54 | 1.04
fcl [20] 76.21 61.30 311.06 39.26 | 1.76

 

 

 

 

 

 

this state-of-the-art network structure, our method can reduce nearly half of the
parameters and significantly reduce the memory footprint at the expenses of a
slight drop in performance.

 

7 Conclusion

We proposed an algorithm to significantly reduce of the number of neurons in
a convolutional neural network by adding sparse constraints during the training
step. The forward-backward splitting method is applied to solve the sparse con-
strained problem. We also analyze the benefits of using rectified linear units as
non-linear activation function to remove a larger number of neurons.

Experiments using four popular CNNs including AlexNet and VGG-B demon-
strate the capacity of the proposed method to reduce the number of neurons,
therefore, the number of parameters and memory footprint, with a negligible
loss in performance.

Acknowledgment The authors thank NVIDIA for generous hardware donations.
Less is More: Towards Compact CNNs 15

References

10.

11.

12.

13.

14.

is

18.

19.

20.

21.

22.

. Srinivas, S., Babu, R.V.: Data-free parameter pruning for deep neural networks.

In: BMVC. (2015) 1, 3, 4, 11, 13

Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. CoRR abs/1409.1556 (2014) 1, 8

Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep
convolutional neural networks. In: NIPS. (2012) 1, 7, 8, 13

Denton, E.L., Zaremba, W., Bruna, J., LeCun, Y., Fergus, R.: Exploiting linear
structure within convolutional networks for efficient evaluation. In: NIPS. (2014)
2,3
Jaderberg, M., Vedaldi, A., Zisserman, A.: Speeding up convolutional neural net-
works with low rank expansions. In: BMVC. (2014) 2, 3

Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadar-
rama, S., Darrell, T.: Caffe: Convolutional architecture for fast feature embedding.
arXiv preprint arXiv:1408.5093 (2014) 2, 8

Collobert, R., Kavukcuoglu, K., Farabet, C.: Torch7: A matlab-like environment
for machine learning. In: BigLearn, NIPS Workshop. (2011) 2, 8

Vedaldi, A., Lenc, K.: Matconvnet — convolutional neural networks for matlab. In:
ACM MM 2, 8, 9, 12

 

. Mathieu, M., Henaff, M., LeCun, Y.: Fast training of convolutional networks

through ffts. CoRR abs/1312.5851 (2013) 2

Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accu-
rate object detection and semantic segmentation. In: CVPR. (2014) 2

Chen, L., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Semantic image
segmentation with deep convolutional nets and fully connected crfs. In: ICLR.
(2014) 2

Duchi, J., Singer, Y.: Efficient online and batch learning using forward backward
splitting. Journal of Machine Learning Research 10 (2009) 2, 5, 6

Goldstein, T., Studer, C., Baraniuk, R.G.: A field guide to forward-backward
splitting with a FASTA implementation. arXiv eprint abs/1411.3406 (2014) 2,
5, 6

Bucila, C., Caruana, R., Niculescu-Mizil, A.: Model compression. In: ACM
SIGKDD. (2006) 3

. Ba, J., Caruana, R.: Do deep nets really need to be deep? In: NIPS. (2014) 3
. Hinton, G.E., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network.

In: NIPS 2014 Deep Learning Workshop. (2014) 3

Romero, A., Ballas, N., Kahou, S.E., Chassang, A., Gatta, C., Bengio, Y.: Fitnets:
Hints for thin deep nets. In: ICLR. (2015) 3

Denil, M., Shakibi, B., Dinh, L., Ranzato, M., Freitasa, N.D.: Predicting parame-
ters in deep learning. In: NIPS. (2013) 3

Liu, B., Wang, M., Foroosh, H., Tappen, M., Penksy, M.: Sparse convolutional
neural networks. In: CVPR. (2015) 3

Lebedev, V., Ganin, Y., Rakhuba, M., Oseledets, I.V., Lempitsky, V.S.: Speeding-
up convolutional neural networks using fine-tuned cp-decomposition. In: ICLR.
(2015) 4

Gong, Y., Liu, L., Yang, M., Bourdev, L.D.: Compressing deep convolutional
networks using vector quantization. CoRR abs/1412.6115 (2014) 4

Novikov, A., Podoprikhin, D., Osokin, A., Vetrov, D.P.: Tensorizing neural net-
works. In: NIPS. (2015) 4
16

23.
24.

26.

27.

36.

37.

38.

39.

40.

Hao Zhou, Jose M. Alvarez and Fatih Porikli

LeCun, Y., Denker, J.S., Solla, S.A.: Optimal brain damage. In: NIPS. (1990) 4
Hassibi, B., Stork, D.G.: Second order derivatives for network pruning: Optimal
brain surgeon. In: NIPS. (1993) 4

. Han, S., Pool, J., Tran, J., Dally, W.J.: Learning both weights and connections for

efficient neural networks. In: NIPS. (2015) 4

Collins, M.D., Kohli, P.: Memory bounded deep convolutional networks. CoRR
abs/1412.1442 (2014) 4, 5

Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural network
with pruning, trained quantization and huffman coding. In: ICLR. (2016) 4

. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to

document recognition. Proceedings of the IEEE 86(11) (1998) 5, 8

. Yu, D., Seide, F., Li, G., Deng, L.: Exploiting sparseness in deep neural networks

for large vocabulary speech recognition. In: ICASSP. (2012) 5, 6

. Tsuruoka, Y., Tsujii, J.. Ananiadou, S.: Stochastic gradient descent training for

l1-regularized log-linear models with cumulative penalty. In: ACL-IJCNLP. (2009)

5, 6

. Liu, J., Musialski, P., Wonka, P., Ye, J.: Tensor completion for estimating missing

values in visual data. IEEE Transactions on PAMI 35(1) (2013) 6, 7

. Deng, W., Yin, W., Zhang, Y.: Group sparse optimization by alternating direction

method. In: SPIE. Volume 8858. (2013) 6, 7

. Nair, V., Hinton, G.E.: Rectified linear units improve restricted boltzmann ma-

chines. In: ICML. (2010) 8

. Glorot, X., Bordes, A., Bengio, Y.: Deep sparse rectifier neural networks. In:

AISTATS. (2011) 8

. Zeiler, M.D., Ranzato, M., Monga, R., Mao, M., Yang, K., Le, Q., Nguyen, P.,

Senior, A., Vanhoucke, V., Dean, J., Hinton, G.: On rectified linear units for
speech processing. In: ICASSP. (2013) 8

Snoek, J., Larochelle, H., Adams, R.P.: Practical bayesian optimization of machine
learning algorithms. In: NIPS. (2012) 8

Krizhevsky, A.: Learning multiple layers of features from tiny images. Technical
report, Technical report, Department of Computer Science, University of Toronto
(2009) 8, 12

Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: Imagenet large
scale visual recognition challenge. IJCV (2015) 8, 13

Chintala, S.: soumith/imagenet-multigpu.torch.
url=https://github.com/soumith/imagenet-multiGPU.torch (2015) 8, 9, 13
Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale
hierarchical image database. In: CVPR. (2009) 13

